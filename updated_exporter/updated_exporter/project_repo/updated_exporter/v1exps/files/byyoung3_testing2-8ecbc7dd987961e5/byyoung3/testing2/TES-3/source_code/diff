diff --git a/pyproject.toml b/pyproject.toml
index 9a56740..911839f 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -6,10 +6,12 @@ readme = "README.md"
 requires-python = ">=3.13,<3.15"
 dependencies = [
     "click>=8.1.0,<9.0.0",
+    "ffmpeg-python>=0.2.0,<1.0.0",
     "mlflow>=3.4.0,<4.0.0",
     "neptune>=1.14.0,<2.0.0",
     "neptune-query>=1.5.1,<2.0.0",
     "pyarrow>=21.0.0,<22.0.0",
+    "python-magic>=0.4.27,<1.0.0",
     "tqdm>=4.67.1,<5.0.0",
     "wandb>=0.22.3,<1.0.0",
 ]
diff --git a/src/neptune_exporter/loaders/wandb_loader.py b/src/neptune_exporter/loaders/wandb_loader.py
index 3eab064..0070ef2 100644
--- a/src/neptune_exporter/loaders/wandb_loader.py
+++ b/src/neptune_exporter/loaders/wandb_loader.py
@@ -1,3 +1,465 @@
+# #
+# # Copyright (c) 2025, Neptune Labs Sp. z o.o.
+# #
+# # Licensed under the Apache License, Version 2.0 (the "License");
+# # you may not use this file except in compliance with the License.
+# # You may obtain a copy of the License at
+# #
+# #     http://www.apache.org/licenses/LICENSE-2.0
+# #
+# # Unless required by applicable law or agreed to in writing, software
+# # distributed under the License is distributed on an "AS IS" BASIS,
+# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# # See the License for the specific language governing permissions and
+# # limitations under the License.
+
+# import os
+# import re
+# import logging
+# import tempfile
+# from decimal import Decimal
+# from pathlib import Path
+# from typing import Generator, Optional, Any
+# import pandas as pd
+# import pyarrow as pa
+# import wandb
+
+# from neptune_exporter.types import ProjectId, TargetRunId, TargetExperimentId
+# from neptune_exporter.loaders.loader import DataLoader
+
+
+# class WandBLoader(DataLoader):
+#     """Loads Neptune data from parquet files into Weights & Biases."""
+
+#     def __init__(
+#         self,
+#         entity: str,
+#         api_key: Optional[str] = None,
+#         name_prefix: Optional[str] = None,
+#         show_client_logs: bool = False,
+#     ):
+#         """
+#         Initialize W&B loader.
+
+#         Args:
+#             entity: W&B entity (organization/username)
+#             api_key: Optional W&B API key for authentication
+#             name_prefix: Optional prefix for project and run names
+#             verbose: Enable verbose logging
+#         """
+#         self.entity = entity
+#         self.name_prefix = name_prefix
+#         self._logger = logging.getLogger(__name__)
+#         self._active_run: Optional[wandb.Run] = None
+
+#         # Authenticate with W&B
+#         if api_key:
+#             wandb.login(key=api_key)
+
+#         # Configure W&B logging
+#         if not show_client_logs:
+#             os.environ["WANDB_SILENT"] = "true"
+
+#     def _sanitize_attribute_name(self, attribute_path: str) -> str:
+#         """
+#         Sanitize Neptune attribute path to W&B-compatible key.
+
+#         W&B key constraints:
+#         - Must start with a letter or underscore
+#         - Can only contain letters, numbers, and underscores
+#         - Pattern: /^[_a-zA-Z][_a-zA-Z0-9]*$/
+#         """
+#         # Replace invalid characters with underscores
+#         sanitized = re.sub(r"[^a-zA-Z0-9_]", "_", attribute_path)
+
+#         # Ensure it starts with a letter or underscore
+#         if sanitized and not sanitized[0].isalpha() and sanitized[0] != "_":
+#             sanitized = "_" + sanitized
+
+#         # Handle empty result
+#         if not sanitized:
+#             sanitized = "_attribute"
+
+#         return sanitized
+
+#     def _get_project_name(self, project_id: str) -> str:
+#         """Get W&B project name from Neptune project ID."""
+#         # W&B uses entity/project structure
+#         # Neptune project_id maps directly to W&B project
+#         name = project_id
+
+#         if self.name_prefix:
+#             name = f"{self.name_prefix}_{name}"
+
+#         # Sanitize for W&B project name (alphanumeric, hyphens, underscores)
+#         name = re.sub(r"[^a-zA-Z0-9_-]", "_", name)
+
+#         return name
+
+#     def _convert_step_to_int(self, step: Decimal, step_multiplier: int) -> int:
+#         """Convert Neptune decimal step to W&B integer step."""
+#         if step is None:
+#             return 0
+#         return int(float(step) * step_multiplier)
+
+#     def create_experiment(
+#         self, project_id: str, experiment_name: str
+#     ) -> TargetExperimentId:
+#         """
+#         Neptune experiment_name maps to W&B group (set in create_run).
+#         We return the experiment name as the group name to use.
+#         """
+#         return TargetExperimentId(experiment_name)
+
+#     def find_run(
+#         self,
+#         project_id: ProjectId,
+#         run_name: str,
+#         experiment_id: Optional[TargetExperimentId],
+#     ) -> Optional[TargetRunId]:
+#         """Find a run by name in a W&B project.
+
+#         Args:
+#             run_name: Name of the run to find
+#             experiment_id: W&B group name (experiment name from Neptune)
+#             project_id: Neptune project ID (used to construct W&B project name)
+
+#         Returns:
+#             W&B run ID if found, None otherwise
+#         """
+#         sanitized_project = self._get_project_name(project_id)
+
+#         try:
+#             # Use wandb.Api() to search for runs
+#             api = wandb.Api()
+#             project_path = f"{self.entity}/{sanitized_project}"
+
+#             # Search for runs with matching name and group
+#             filters = {"display_name": run_name}
+#             if experiment_id:
+#                 filters["group"] = experiment_id
+
+#             runs = api.runs(project_path, filters=filters, per_page=1)
+
+#             # Get the first matching run
+#             for run in runs:
+#                 return TargetRunId(run.id)
+
+#             return None
+#         except Exception:
+#             self._logger.error(
+#                 f"Error finding project {project_id}, run '{run_name}'",
+#                 exc_info=True,
+#             )
+#             return None
+
+#     def create_run(
+#         self,
+#         project_id: ProjectId,
+#         run_name: str,
+#         experiment_id: Optional[TargetExperimentId] = None,
+#         parent_run_id: Optional[TargetRunId] = None,
+#         fork_step: Optional[float] = None,
+#         step_multiplier: Optional[int] = None,
+#     ) -> TargetRunId:
+#         """Create W&B run, with support for forked runs.
+
+#         Args:
+#             fork_step: Fork step as float (decimal). Will be converted to int using step_multiplier.
+#             step_multiplier: Step multiplier for converting decimal steps to integers.
+#                 If provided, will be used for fork_step conversion. If not provided,
+#                 will calculate from fork_step alone as fallback.
+#         """
+#         sanitized_project = self._get_project_name(project_id)
+
+#         try:
+#             # Prepare init arguments
+#             init_kwargs: dict[str, Any] = {
+#                 "entity": self.entity,
+#                 "project": sanitized_project,
+#                 "group": experiment_id,
+#                 "name": run_name,
+#             }
+
+#             # Handle forking if parent exists
+#             if parent_run_id:
+#                 # Convert fork_step to int using provided step_multiplier
+#                 # step_multiplier should always be provided when fork_step is set
+#                 if fork_step is not None:
+#                     if step_multiplier is None:
+#                         raise ValueError(
+#                             "step_multiplier must be provided when fork_step is set"
+#                         )
+#                     step_int = self._convert_step_to_int(
+#                         Decimal(str(fork_step)), step_multiplier
+#                     )
+#                 else:
+#                     step_int = 0
+
+#                 # W&B fork format: entity/project/run_id?_step=step
+#                 fork_from = f"{self.entity}/{sanitized_project}/{parent_run_id}?_step={step_int}"
+#                 init_kwargs["fork_from"] = fork_from
+#                 self._logger.info(
+#                     f"Creating forked run '{run_name}' from parent {parent_run_id} at step {step_int}"
+#                 )
+
+#             # Initialize the run
+#             run = wandb.init(**init_kwargs)
+#             wandb_run_id = run.id
+
+#             self._active_run = run
+
+#             self._logger.info(f"Created run '{run_name}' with W&B ID {wandb_run_id}")
+#             return TargetRunId(wandb_run_id)
+
+#         except Exception:
+#             self._logger.error(
+#                 f"Error creating project {project_id}, run '{run_name}'",
+#                 exc_info=True,
+#             )
+#             raise
+
+#     def upload_run_data(
+#         self,
+#         run_data: Generator[pa.Table, None, None],
+#         run_id: TargetRunId,
+#         files_directory: Path,
+#         step_multiplier: int,
+#     ) -> None:
+#         """Upload all data for a single run to W&B.
+
+#         Args:
+#             step_multiplier: Step multiplier for converting decimal steps to integers
+#         """
+#         try:
+#             # Note: We assume the run is already active from create_run
+#             # If not, we would need to resume it
+#             if self._active_run is None or self._active_run.id != run_id:
+#                 self._logger.error(
+#                     f"Run {run_id} is not active. Call create_run first."
+#                 )
+#                 raise RuntimeError(f"Run {run_id} is not active")
+
+#             for run_data_part in run_data:
+#                 run_df = run_data_part.to_pandas()
+
+#                 self.upload_parameters(run_df, run_id)
+#                 self.upload_metrics(run_df, run_id, step_multiplier)
+#                 self.upload_artifacts(run_df, run_id, files_directory, step_multiplier)
+
+#             # Finish the run
+#             self._active_run.finish()
+#             self._active_run = None
+
+#             self._logger.info(f"Successfully uploaded run {run_id} to W&B")
+
+#         except Exception:
+#             self._logger.error(f"Error uploading data for run {run_id}", exc_info=True)
+#             if self._active_run:
+#                 self._active_run.finish(exit_code=1)
+#                 self._active_run = None
+#             raise
+
+#     def upload_parameters(self, run_data: pd.DataFrame, run_id: TargetRunId) -> None:
+#         """Upload parameters (configs) to W&B run."""
+#         if self._active_run is None:
+#             raise RuntimeError("No active run")
+
+#         param_types = {"float", "int", "string", "bool", "datetime", "string_set"}
+#         param_data = run_data[run_data["attribute_type"].isin(param_types)]
+
+#         if param_data.empty:
+#             return
+
+#         config = {}
+#         for _, row in param_data.iterrows():
+#             attr_name = self._sanitize_attribute_name(row["attribute_path"])
+
+#             # Get the appropriate value based on attribute type
+#             if row["attribute_type"] == "float" and pd.notna(row["float_value"]):
+#                 config[attr_name] = row["float_value"]
+#             elif row["attribute_type"] == "int" and pd.notna(row["int_value"]):
+#                 config[attr_name] = int(row["int_value"])
+#             elif row["attribute_type"] == "string" and pd.notna(row["string_value"]):
+#                 config[attr_name] = row["string_value"]
+#             elif row["attribute_type"] == "bool" and pd.notna(row["bool_value"]):
+#                 config[attr_name] = bool(row["bool_value"])
+#             elif row["attribute_type"] == "datetime" and pd.notna(
+#                 row["datetime_value"]
+#             ):
+#                 config[attr_name] = str(row["datetime_value"])
+#             elif (
+#                 row["attribute_type"] == "string_set"
+#                 and row["string_set_value"] is not None
+#             ):
+#                 config[attr_name] = list(row["string_set_value"])
+
+#         if config:
+#             self._active_run.config.update(config)
+#             self._logger.info(f"Uploaded {len(config)} parameters for run {run_id}")
+
+#     def upload_metrics(
+#         self, run_data: pd.DataFrame, run_id: TargetRunId, step_multiplier: int
+#     ) -> None:
+#         """Upload metrics (float series) to W&B run.
+
+#         Args:
+#             step_multiplier: Global step multiplier for the run (calculated from all series + fork_step)
+#         """
+#         if self._active_run is None:
+#             raise RuntimeError("No active run")
+
+#         metrics_data = run_data[run_data["attribute_type"] == "float_series"]
+
+#         if metrics_data.empty:
+#             return
+
+#         # Use global step multiplier (calculated from all series + fork_step)
+#         # Group by step to log all metrics at each step together
+#         for step_value, group in metrics_data.groupby("step"):
+#             if pd.notna(step_value):
+#                 step = self._convert_step_to_int(step_value, step_multiplier)
+
+#                 metrics = {}
+#                 for _, row in group.iterrows():
+#                     if pd.notna(row["float_value"]):
+#                         attr_name = self._sanitize_attribute_name(row["attribute_path"])
+#                         metrics[attr_name] = row["float_value"]
+
+#                 if metrics:
+#                     self._active_run.log(metrics, step=step)
+
+#         self._logger.info(f"Uploaded metrics for run {run_id}")
+
+#     def upload_artifacts(
+#         self,
+#         run_data: pd.DataFrame,
+#         run_id: TargetRunId,
+#         files_base_path: Path,
+#         step_multiplier: int,
+#     ) -> None:
+#         """Upload files and series as artifacts to W&B run.
+
+#         Args:
+#             step_multiplier: Global step multiplier for the run (calculated from all series + fork_step)
+#         """
+#         if self._active_run is None:
+#             raise RuntimeError("No active run")
+
+#         # Handle regular files
+#         file_data = run_data[
+#             run_data["attribute_type"].isin(["file", "file_set", "artifact"])
+#         ]
+#         for _, row in file_data.iterrows():
+#             if pd.notna(row["file_value"]) and isinstance(row["file_value"], dict):
+#                 file_path = files_base_path / row["file_value"]["path"]
+#                 if file_path.exists():
+#                     attr_name = self._sanitize_attribute_name(row["attribute_path"])
+#                     artifact = wandb.Artifact(
+#                         name=attr_name, type=row["attribute_type"]
+#                     )
+#                     if file_path.is_file():
+#                         artifact.add_file(str(file_path))
+#                     else:
+#                         artifact.add_dir(str(file_path))
+#                     self._active_run.log_artifact(artifact)
+#                 else:
+#                     self._logger.warning(f"File not found: {file_path}")
+
+#         # Handle file series
+#         file_series_data = run_data[run_data["attribute_type"] == "file_series"]
+#         for attr_path, group in file_series_data.groupby("attribute_path"):
+#             attr_name = self._sanitize_attribute_name(attr_path)
+
+#             for _, row in group.iterrows():
+#                 if pd.notna(row["file_value"]) and isinstance(row["file_value"], dict):
+#                     file_path = files_base_path / row["file_value"]["path"]
+#                     if file_path.exists():
+#                         step = (
+#                             self._convert_step_to_int(row["step"], step_multiplier)
+#                             if pd.notna(row["step"])
+#                             else 0
+#                         )
+#                         artifact_name = f"{attr_name}_step_{step}"
+#                         artifact = wandb.Artifact(
+#                             name=artifact_name, type="file_series"
+#                         )
+#                         if file_path.is_file():
+#                             artifact.add_file(str(file_path))
+#                         else:
+#                             artifact.add_dir(str(file_path))
+#                         self._active_run.log_artifact(artifact)
+#                     else:
+#                         self._logger.warning(f"File not found: {file_path}")
+
+#         # Handle string series as text artifacts
+#         string_series_data = run_data[run_data["attribute_type"] == "string_series"]
+#         for attr_path, group in string_series_data.groupby("attribute_path"):
+#             attr_name = self._sanitize_attribute_name(attr_path)
+
+#             # Create temporary file with text content
+#             with tempfile.NamedTemporaryFile(
+#                 mode="w", suffix=".txt", encoding="utf-8"
+#             ) as tmp_file:
+#                 for _, row in group.iterrows():
+#                     if pd.notna(row["string_value"]):
+#                         series_step = (
+#                             self._convert_step_to_int(row["step"], step_multiplier)
+#                             if pd.notna(row["step"])
+#                             else None
+#                         )
+#                         timestamp = (
+#                             row["timestamp"].isoformat()
+#                             if pd.notna(row["timestamp"])
+#                             else None
+#                         )
+#                         text_line = (
+#                             f"{series_step}; {timestamp}; {row['string_value']}\n"
+#                         )
+#                         tmp_file.write(text_line)
+#                 tmp_file_path = tmp_file.name
+
+#                 # Create and log W&B artifact
+#                 artifact = wandb.Artifact(name=attr_name, type="string_series")
+#                 artifact.add_file(tmp_file_path, name="series.txt")
+#                 self._active_run.log_artifact(artifact)
+
+#         # Handle histogram series as W&B Histograms
+#         histogram_series_data = run_data[
+#             run_data["attribute_type"] == "histogram_series"
+#         ]
+#         for attr_path, group in histogram_series_data.groupby("attribute_path"):
+#             attr_name = self._sanitize_attribute_name(attr_path)
+#             # Use global step multiplier
+
+#             for _, row in group.iterrows():
+#                 if pd.notna(row["histogram_value"]) and isinstance(
+#                     row["histogram_value"], dict
+#                 ):
+#                     step = (
+#                         self._convert_step_to_int(row["step"], step_multiplier)
+#                         if pd.notna(row["step"])
+#                         else 0
+#                     )
+#                     hist = row["histogram_value"]
+
+#                     # Convert Neptune histogram to W&B Histogram
+#                     # Neptune format: {"type": str, "edges": list, "values": list}
+#                     # W&B expects histogram data as np_histogram tuple or sequence
+#                     try:
+#                         wandb_hist = wandb.Histogram(
+#                             np_histogram=(hist.get("values", []), hist.get("edges", []))
+#                         )
+#                         self._active_run.log({attr_name: wandb_hist}, step=step)
+#                     except Exception:
+#                         self._logger.error(
+#                             f"Failed to log histogram for {attr_path} at step {step}",
+#                             exc_info=True,
+#                         )
+
+#         self._logger.info(f"Uploaded artifacts for run {run_id}")
+
+
 #
 # Copyright (c) 2025, Neptune Labs Sp. z o.o.
 #
@@ -23,11 +485,21 @@ from typing import Generator, Optional, Any
 import pandas as pd
 import pyarrow as pa
 import wandb
+import magic
+import ffmpeg
 
 from neptune_exporter.types import ProjectId, TargetRunId, TargetExperimentId
 from neptune_exporter.loaders.loader import DataLoader
 
 
+# Rich media file extensions
+IMAGE_EXTENSIONS = {".png", ".jpg", ".jpeg", ".gif", ".bmp", ".webp", ".svg", ".tiff", ".tif"}
+AUDIO_EXTENSIONS = {".wav", ".mp3", ".ogg", ".flac", ".aac", ".m4a"}
+VIDEO_EXTENSIONS = {".mp4", ".avi", ".mov", ".mkv", ".webm"}
+TABLE_EXTENSIONS = {".csv", ".tsv"}
+HTML_EXTENSIONS = {".html", ".htm"}
+
+
 class WandBLoader(DataLoader):
     """Loads Neptune data from parquet files into Weights & Biases."""
 
@@ -37,6 +509,7 @@ class WandBLoader(DataLoader):
         api_key: Optional[str] = None,
         name_prefix: Optional[str] = None,
         show_client_logs: bool = False,
+        rich: bool = False,
     ):
         """
         Initialize W&B loader.
@@ -45,10 +518,12 @@ class WandBLoader(DataLoader):
             entity: W&B entity (organization/username)
             api_key: Optional W&B API key for authentication
             name_prefix: Optional prefix for project and run names
-            verbose: Enable verbose logging
+            show_client_logs: Enable verbose logging
+            rich: Upload files as native W&B media types (images, audio, video, tables)
         """
         self.entity = entity
         self.name_prefix = name_prefix
+        self.rich = rich
         self._logger = logging.getLogger(__name__)
         self._active_run: Optional[wandb.Run] = None
 
@@ -60,6 +535,248 @@ class WandBLoader(DataLoader):
         if not show_client_logs:
             os.environ["WANDB_SILENT"] = "true"
 
+        if self.rich:
+            self._logger.info("Rich mode enabled - uploading as native W&B media types")
+
+    def _get_file_type(self, file_path: Path) -> str:
+        """Determine file type using extension, magic bytes, and directory hints."""
+        ext = file_path.suffix.lower()
+        print(f"[DEBUG _get_file_type] file_path: {file_path}, ext: '{ext}'")
+
+        # Step 1: Try extension first (fastest)
+        if ext in IMAGE_EXTENSIONS:
+            print(f"[DEBUG _get_file_type] Matched IMAGE by extension")
+            return "image"
+        elif ext in AUDIO_EXTENSIONS:
+            print(f"[DEBUG _get_file_type] Matched AUDIO by extension")
+            return "audio"
+        elif ext in VIDEO_EXTENSIONS:
+            print(f"[DEBUG _get_file_type] Matched VIDEO by extension")
+            return "video"
+        elif ext in TABLE_EXTENSIONS:
+            print(f"[DEBUG _get_file_type] Matched TABLE by extension")
+            return "table"
+        elif ext in HTML_EXTENSIONS:
+            print(f"[DEBUG _get_file_type] Matched HTML by extension")
+            return "html"
+
+        # Step 2: No extension or unknown extension - use magic bytes
+        print(f"[DEBUG _get_file_type] No extension match, trying magic bytes...")
+        try:
+            mime = magic.from_file(str(file_path), mime=True)
+            print(f"[DEBUG _get_file_type] Detected MIME type: {mime}")
+
+            # Map MIME types to our categories
+            if mime.startswith('image/'):
+                # Special case: GIFs might be in video/ folder but are images for W&B
+                parent_dir = file_path.parent.name
+                if mime == 'image/gif' and parent_dir == 'video':
+                    print(f"[DEBUG _get_file_type] GIF in video folder - treating as video")
+                    return "video"
+                print(f"[DEBUG _get_file_type] Matched IMAGE by magic")
+                return "image"
+            elif mime.startswith('audio/'):
+                print(f"[DEBUG _get_file_type] Matched AUDIO by magic")
+                return "audio"
+            elif mime.startswith('video/'):
+                print(f"[DEBUG _get_file_type] Matched VIDEO by magic")
+                return "video"
+            elif mime in ('text/csv', 'text/tab-separated-values'):
+                print(f"[DEBUG _get_file_type] Matched TABLE by magic")
+                return "table"
+            elif mime == 'text/html':
+                print(f"[DEBUG _get_file_type] Matched HTML by magic")
+                return "html"
+        except Exception as e:
+            print(f"[DEBUG _get_file_type] Magic bytes detection failed: {e}")
+
+        # Step 3: Use directory name as hint (common sense!)
+        parent_dir = file_path.parent.name
+        print(f"[DEBUG _get_file_type] Trying directory hint: {parent_dir}")
+
+        if parent_dir in ('images', 'image', 'visualizations'):
+            print(f"[DEBUG _get_file_type] Directory suggests IMAGE")
+            return "image"
+        elif parent_dir == 'audio':
+            print(f"[DEBUG _get_file_type] Directory suggests AUDIO")
+            return "audio"
+        elif parent_dir == 'video':
+            print(f"[DEBUG _get_file_type] Directory suggests VIDEO")
+            return "video"
+        elif parent_dir in ('tables', 'table'):
+            print(f"[DEBUG _get_file_type] Directory suggests TABLE")
+            return "table"
+
+        print(f"[DEBUG _get_file_type] No match - returning 'file'")
+        return "file"
+
+    def _reencode_video_to_h264(self, input_path: Path) -> Path:
+        """Re-encode video to H.264 MP4 for browser compatibility.
+
+        Returns path to the re-encoded video (a temp file).
+        """
+        print(f"[DEBUG _reencode_video_to_h264] Re-encoding video: {input_path}")
+
+        # Create temp file for output
+        temp_file = tempfile.NamedTemporaryFile(suffix='.mp4', delete=False)
+        temp_file.close()
+        output_path = temp_file.name
+
+        try:
+            # Re-encode to H.264 with web-compatible settings
+            (
+                ffmpeg
+                .input(str(input_path))
+                .output(
+                    output_path,
+                    vcodec='libx264',  # H.264 codec
+                    acodec='aac',      # AAC audio codec
+                    **{
+                        'preset': 'fast',           # Fast encoding
+                        'crf': '23',                # Quality (lower = better, 23 is good)
+                        'movflags': '+faststart',   # Enable web streaming
+                        'pix_fmt': 'yuv420p',       # Compatible pixel format
+                    }
+                )
+                .overwrite_output()
+                .run(quiet=True, capture_stdout=True, capture_stderr=True)
+            )
+            print(f"[DEBUG _reencode_video_to_h264] Successfully re-encoded to: {output_path}")
+            return Path(output_path)
+        except ffmpeg.Error as e:
+            print(f"[DEBUG _reencode_video_to_h264] FFmpeg error: {e.stderr.decode() if e.stderr else str(e)}")
+            # Clean up on failure
+            if Path(output_path).exists():
+                os.unlink(output_path)
+            raise
+
+    def _get_extension_for_mime(self, mime: str) -> str:
+        """Map MIME type to file extension for W&B."""
+        mime_to_ext = {
+            # Images
+            'image/png': '.png',
+            'image/jpeg': '.jpg',
+            'image/gif': '.gif',
+            'image/bmp': '.bmp',
+            'image/webp': '.webp',
+            'image/svg+xml': '.svg',
+            'image/tiff': '.tiff',
+            # Audio
+            'audio/mpeg': '.mp3',
+            'audio/wav': '.wav',
+            'audio/wave': '.wav',
+            'audio/x-wav': '.wav',
+            'audio/ogg': '.ogg',
+            'audio/flac': '.flac',
+            'audio/aac': '.aac',
+            'audio/mp4': '.m4a',
+            # Video
+            'video/mp4': '.mp4',
+            'video/mpeg': '.mpeg',
+            'video/quicktime': '.mov',
+            'video/x-msvideo': '.avi',
+            'video/webm': '.webm',
+            'video/x-matroska': '.mkv',
+        }
+        return mime_to_ext.get(mime, '')
+
+    def _upload_rich_file(self, file_path: Path, attr_name: str, step: Optional[int] = None) -> bool:
+        """Upload file as native W&B type. Returns True if successful."""
+        print(f"\n[DEBUG _upload_rich_file] Called with:")
+        print(f"  - file_path: {file_path}")
+        print(f"  - attr_name: {attr_name}")
+        print(f"  - step: {step}")
+        print(f"  - file exists: {file_path.exists()}")
+        print(f"  - is file: {file_path.is_file() if file_path.exists() else 'N/A'}")
+
+        if not file_path.exists() or not file_path.is_file():
+            print(f"[DEBUG _upload_rich_file] File doesn't exist or isn't a file - returning False")
+            return False
+
+        file_type = self._get_file_type(file_path)
+        print(f"[DEBUG _upload_rich_file] Detected file_type: {file_type}")
+
+        # Check if file already has an extension
+        has_extension = bool(file_path.suffix)
+        temp_file = None
+        reencoded_video = None
+
+        try:
+            # For audio/video without extensions, W&B needs the extension in the filename
+            # Create a temp file with proper extension
+            if not has_extension and file_type in ('audio', 'video'):
+                print(f"[DEBUG _upload_rich_file] File has no extension, detecting MIME type...")
+                mime = magic.from_file(str(file_path), mime=True)
+                ext = self._get_extension_for_mime(mime)
+                print(f"[DEBUG _upload_rich_file] MIME: {mime}, Extension: {ext}")
+
+                if ext:
+                    # Create temp file with proper extension
+                    import shutil
+                    temp_file = tempfile.NamedTemporaryFile(suffix=ext, delete=False)
+                    temp_file.close()
+                    shutil.copy2(file_path, temp_file.name)
+                    file_path = Path(temp_file.name)
+                    print(f"[DEBUG _upload_rich_file] Created temp file with extension: {file_path}")
+                else:
+                    print(f"[DEBUG _upload_rich_file] Could not determine extension for MIME: {mime}")
+                    return False
+
+            if file_type == "image":
+                print(f"[DEBUG _upload_rich_file] Creating wandb.Image for {file_path}")
+                media = wandb.Image(str(file_path))
+            elif file_type == "audio":
+                print(f"[DEBUG _upload_rich_file] Creating wandb.Audio for {file_path}")
+                media = wandb.Audio(str(file_path))
+            elif file_type == "video":
+                # Re-encode video to H.264 for browser compatibility
+                print(f"[DEBUG _upload_rich_file] Re-encoding video to H.264 for browser compatibility...")
+                reencoded_video = self._reencode_video_to_h264(file_path)
+                print(f"[DEBUG _upload_rich_file] Creating wandb.Video for {reencoded_video}")
+                media = wandb.Video(str(reencoded_video))
+            elif file_type == "table":
+                print(f"[DEBUG _upload_rich_file] Creating wandb.Table for {file_path}")
+                df = pd.read_csv(file_path) if file_path.suffix == ".csv" else pd.read_csv(file_path, sep="\t")
+                media = wandb.Table(dataframe=df)
+            elif file_type == "html":
+                print(f"[DEBUG _upload_rich_file] Creating wandb.Html for {file_path}")
+                with open(file_path, "r", encoding="utf-8") as f:
+                    media = wandb.Html(f.read())
+            else:
+                print(f"[DEBUG _upload_rich_file] Unknown file_type '{file_type}' - returning False")
+                return False
+
+            print(f"[DEBUG _upload_rich_file] Logging media to wandb with attr_name: {attr_name}")
+            if step is not None:
+                self._active_run.log({attr_name: media, "step": step})
+            else:
+                self._active_run.log({attr_name: media})
+            print(f"[DEBUG _upload_rich_file] Successfully logged! Returning True")
+            return True
+
+        except Exception as e:
+            print(f"[DEBUG _upload_rich_file] EXCEPTION caught: {e}")
+            print(f"[DEBUG _upload_rich_file] Exception type: {type(e)}")
+            import traceback
+            print(f"[DEBUG _upload_rich_file] Traceback:\n{traceback.format_exc()}")
+            self._logger.warning(f"Failed to upload rich file {file_path}: {e}")
+            return False
+        finally:
+            # Clean up temp files if created
+            if temp_file and Path(temp_file.name).exists():
+                print(f"[DEBUG _upload_rich_file] Cleaning up temp file: {temp_file.name}")
+                try:
+                    os.unlink(temp_file.name)
+                except Exception as e:
+                    print(f"[DEBUG _upload_rich_file] Failed to delete temp file: {e}")
+
+            if reencoded_video and reencoded_video.exists():
+                print(f"[DEBUG _upload_rich_file] Cleaning up re-encoded video: {reencoded_video}")
+                try:
+                    os.unlink(reencoded_video)
+                except Exception as e:
+                    print(f"[DEBUG _upload_rich_file] Failed to delete re-encoded video: {e}")
+
     def _sanitize_attribute_name(self, attribute_path: str) -> str:
         """
         Sanitize Neptune attribute path to W&B-compatible key.
@@ -205,6 +922,8 @@ class WandBLoader(DataLoader):
 
             # Initialize the run
             run = wandb.init(**init_kwargs)
+            # run.define_metric("*", step_sync=False)
+
             wandb_run_id = run.id
 
             self._active_run = run
@@ -350,11 +1069,29 @@ class WandBLoader(DataLoader):
         file_data = run_data[
             run_data["attribute_type"].isin(["file", "file_set", "artifact"])
         ]
+        print(f"\n[DEBUG upload_artifacts] Processing {len(file_data)} regular files")
+        print(f"[DEBUG upload_artifacts] self.rich = {self.rich}")
+
         for _, row in file_data.iterrows():
             if pd.notna(row["file_value"]) and isinstance(row["file_value"], dict):
                 file_path = files_base_path / row["file_value"]["path"]
+                print(f"\n[DEBUG upload_artifacts] Processing file: {file_path}")
+                print(f"[DEBUG upload_artifacts] attribute_path: {row['attribute_path']}")
+                print(f"[DEBUG upload_artifacts] attribute_type: {row['attribute_type']}")
                 if file_path.exists():
                     attr_name = self._sanitize_attribute_name(row["attribute_path"])
+
+                    # Try rich upload first if enabled
+                    if self.rich and file_path.is_file():
+                        print(f"[DEBUG upload_artifacts] Attempting rich upload...")
+                        if self._upload_rich_file(file_path, attr_name):
+                            print(f"[DEBUG upload_artifacts] Rich upload succeeded, skipping artifact")
+                            continue
+                        print(f"[DEBUG upload_artifacts] Rich upload failed, falling back to artifact")
+                    else:
+                        print(f"[DEBUG upload_artifacts] Skipping rich upload (rich={self.rich}, is_file={file_path.is_file()})")
+
+                    # Fall back to artifact
                     artifact = wandb.Artifact(
                         name=attr_name, type=row["attribute_type"]
                     )
@@ -368,9 +1105,127 @@ class WandBLoader(DataLoader):
 
         # Handle file series
         file_series_data = run_data[run_data["attribute_type"] == "file_series"]
+        print(f"\n[DEBUG upload_artifacts] Processing {len(file_series_data)} file_series items")
+
         for attr_path, group in file_series_data.groupby("attribute_path"):
             attr_name = self._sanitize_attribute_name(attr_path)
+            print(f"\n[DEBUG upload_artifacts] Processing file_series group: {attr_path}")
+            print(f"[DEBUG upload_artifacts] Group has {len(group)} items")
+
+            # Try rich mode: collect all files and create a table
+            if self.rich:
+                print(f"[DEBUG upload_artifacts] Rich mode: collecting files for table...")
+                table_rows = []
+                all_files_exist = True
+                file_type = None
 
+                for _, row in group.iterrows():
+                    if pd.notna(row["file_value"]) and isinstance(row["file_value"], dict):
+                        file_path = files_base_path / row["file_value"]["path"]
+                        if file_path.exists() and file_path.is_file():
+                            step = (
+                                self._convert_step_to_int(row["step"], step_multiplier)
+                                if pd.notna(row["step"])
+                                else 0
+                            )
+
+                            # Detect file type from first file
+                            if file_type is None:
+                                file_type = self._get_file_type(file_path)
+                                print(f"[DEBUG upload_artifacts] Detected file_series type: {file_type}")
+
+                            table_rows.append({
+                                "step": step,
+                                "file_path": file_path,
+                                "row": row
+                            })
+                        else:
+                            all_files_exist = False
+                            self._logger.warning(f"File not found or not a file: {file_path}")
+
+                # If we have files and they're media (image/video/audio), create a table
+                if table_rows and file_type in ("image", "video", "audio") and all_files_exist:
+                    print(f"[DEBUG upload_artifacts] Creating wandb.Table for {len(table_rows)} {file_type}s")
+                    try:
+                        # Sort by step
+                        table_rows.sort(key=lambda x: x["step"])
+
+                        # Create table with descriptive column name
+                        if file_type == "image":
+                            table = wandb.Table(columns=["step", "image"])
+                        elif file_type == "video":
+                            table = wandb.Table(columns=["step", "video"])
+                        elif file_type == "audio":
+                            table = wandb.Table(columns=["step", "audio"])
+                        else:
+                            table = wandb.Table(columns=["step", "file"])
+
+                        temp_files_to_cleanup = []  # Collect temp files to delete after logging
+
+                        for row_data in table_rows:
+                            file_path = row_data["file_path"]
+                            step = row_data["step"]
+
+                            # Ensure step is a plain Python int
+                            step_int = int(step)
+
+                            # Create appropriate wandb media object and add to table
+                            # Use EXACT same pattern as upload_media.py
+                            if file_type == "image":
+                                print(f"[DEBUG] Creating wandb.Image from: {file_path}")
+                                from PIL import Image as PILImage
+                                import numpy as np
+                                img = PILImage.open(file_path)
+                                # Convert to numpy array to force loading (PIL.open is lazy)
+                                img_array = np.array(img)
+                                table.add_data(step_int, wandb.Image(img_array, caption=f"Step {step_int}"))
+                                print(f"[DEBUG] Added image at step {step_int} to table")
+                            elif file_type == "video":
+                                print(f"[DEBUG] Creating wandb.Video from: {file_path}")
+                                # Re-encode video for browser compatibility
+                                reencoded = self._reencode_video_to_h264(file_path)
+                                table.add_data(step_int, wandb.Video(str(reencoded), caption=f"Step {step_int}"))
+                                # Track temp file for cleanup AFTER table is logged
+                                temp_files_to_cleanup.append(reencoded)
+                                print(f"[DEBUG] Added video at step {step_int} to table")
+                            elif file_type == "audio":
+                                print(f"[DEBUG] Creating wandb.Audio from: {file_path}")
+                                # Copy to temp file with proper extension if needed
+                                if not file_path.suffix:
+                                    import shutil
+                                    mime = magic.from_file(str(file_path), mime=True)
+                                    ext = self._get_extension_for_mime(mime)
+                                    temp_file = tempfile.NamedTemporaryFile(suffix=ext, delete=False)
+                                    temp_file.close()
+                                    shutil.copy2(file_path, temp_file.name)
+                                    table.add_data(step_int, wandb.Audio(temp_file.name, caption=f"Step {step_int}"))
+                                    # Track temp file for cleanup AFTER table is logged
+                                    temp_files_to_cleanup.append(Path(temp_file.name))
+                                else:
+                                    table.add_data(step_int, wandb.Audio(str(file_path), caption=f"Step {step_int}"))
+                                print(f"[DEBUG] Added audio at step {step_int} to table")
+
+                        # Log the table FIRST (no step parameter - table IS the series)
+                        self._active_run.log({attr_name: table})
+                        print(f"[DEBUG upload_artifacts] Successfully logged table with {len(table_rows)} items")
+
+                        # NOW clean up temp files
+                        for temp_path in temp_files_to_cleanup:
+                            if temp_path.exists():
+                                print(f"[DEBUG upload_artifacts] Cleaning up temp file: {temp_path}")
+                                os.unlink(temp_path)
+
+                        continue  # Skip artifact upload for this group
+
+                    except Exception as e:
+                        print(f"[DEBUG upload_artifacts] Table creation failed: {e}")
+                        import traceback
+                        print(f"[DEBUG upload_artifacts] Traceback:\n{traceback.format_exc()}")
+                        self._logger.warning(f"Failed to create table for {attr_path}: {e}")
+                        # Fall through to artifact upload
+
+            # Fall back to individual artifacts (non-rich mode or table creation failed)
+            print(f"[DEBUG upload_artifacts] Falling back to artifact upload for file_series")
             for _, row in group.iterrows():
                 if pd.notna(row["file_value"]) and isinstance(row["file_value"], dict):
                     file_path = files_base_path / row["file_value"]["path"]
@@ -380,6 +1235,7 @@ class WandBLoader(DataLoader):
                             if pd.notna(row["step"])
                             else 0
                         )
+
                         artifact_name = f"{attr_name}_step_{step}"
                         artifact = wandb.Artifact(
                             name=artifact_name, type="file_series"
@@ -392,15 +1248,14 @@ class WandBLoader(DataLoader):
                     else:
                         self._logger.warning(f"File not found: {file_path}")
 
-        # Handle string series as text artifacts
+        # Handle string series
         string_series_data = run_data[run_data["attribute_type"] == "string_series"]
         for attr_path, group in string_series_data.groupby("attribute_path"):
             attr_name = self._sanitize_attribute_name(attr_path)
 
-            # Create temporary file with text content
-            with tempfile.NamedTemporaryFile(
-                mode="w", suffix=".txt", encoding="utf-8"
-            ) as tmp_file:
+            if self.rich:
+                # Upload as W&B Table
+                table_data = []
                 for _, row in group.iterrows():
                     if pd.notna(row["string_value"]):
                         series_step = (
@@ -413,16 +1268,41 @@ class WandBLoader(DataLoader):
                             if pd.notna(row["timestamp"])
                             else None
                         )
-                        text_line = (
-                            f"{series_step}; {timestamp}; {row['string_value']}\n"
-                        )
-                        tmp_file.write(text_line)
-                tmp_file_path = tmp_file.name
+                        table_data.append({
+                            "step": series_step,
+                            "timestamp": timestamp,
+                            "value": row["string_value"]
+                        })
+                if table_data:
+                    table = wandb.Table(dataframe=pd.DataFrame(table_data))
+                    self._active_run.log({attr_name: table})
+            else:
+                # Original: create text artifact
+                with tempfile.NamedTemporaryFile(
+                    mode="w", suffix=".txt", encoding="utf-8", delete=False
+                ) as tmp_file:
+                    for _, row in group.iterrows():
+                        if pd.notna(row["string_value"]):
+                            series_step = (
+                                self._convert_step_to_int(row["step"], step_multiplier)
+                                if pd.notna(row["step"])
+                                else None
+                            )
+                            timestamp = (
+                                row["timestamp"].isoformat()
+                                if pd.notna(row["timestamp"])
+                                else None
+                            )
+                            text_line = (
+                                f"{series_step}; {timestamp}; {row['string_value']}\n"
+                            )
+                            tmp_file.write(text_line)
+                    tmp_file_path = tmp_file.name
 
-                # Create and log W&B artifact
                 artifact = wandb.Artifact(name=attr_name, type="string_series")
                 artifact.add_file(tmp_file_path, name="series.txt")
                 self._active_run.log_artifact(artifact)
+                os.unlink(tmp_file_path)
 
         # Handle histogram series as W&B Histograms
         histogram_series_data = run_data[
@@ -457,4 +1337,4 @@ class WandBLoader(DataLoader):
                             exc_info=True,
                         )
 
-        self._logger.info(f"Uploaded artifacts for run {run_id}")
+        self._logger.info(f"Uploaded artifacts for run {run_id}")
\ No newline at end of file
diff --git a/src/neptune_exporter/main.py b/src/neptune_exporter/main.py
index 70f0f59..d9ef5cf 100644
--- a/src/neptune_exporter/main.py
+++ b/src/neptune_exporter/main.py
@@ -395,6 +395,11 @@ def export(
     is_flag=True,
     help="Disable progress bar.",
 )
+@click.option(
+    "--rich",
+    is_flag=True,
+    help="Enable rich console output with tables and formatted display.",
+)
 def load(
     data_path: Path,
     files_path: Path,
@@ -409,6 +414,7 @@ def load(
     verbose: bool,
     log_file: Path,
     no_progress: bool,
+    rich: bool,
 ) -> None:
     """Load exported Neptune data from parquet files to target platforms (MLflow or W&B).
 
@@ -513,6 +519,7 @@ def load(
             api_key=wandb_api_key,
             name_prefix=name_prefix,
             show_client_logs=verbose,
+            rich=rich,
         )
         loader_name = "W&B"
     else:
diff --git a/uv.lock b/uv.lock
index c2b709d..7027cb6 100644
--- a/uv.lock
+++ b/uv.lock
@@ -566,6 +566,18 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/e2/c7/562ff39f25de27caec01e4c1e88cbb5fcae5160802ba3d90be33165df24f/fastmcp-2.12.4-py3-none-any.whl", hash = "sha256:56188fbbc1a9df58c537063f25958c57b5c4d715f73e395c41b51550b247d140", size = 329090 },
 ]
 
+[[package]]
+name = "ffmpeg-python"
+version = "0.2.0"
+source = { registry = "https://pypi.org/simple" }
+dependencies = [
+    { name = "future" },
+]
+sdist = { url = "https://files.pythonhosted.org/packages/dd/5e/d5f9105d59c1325759d838af4e973695081fbbc97182baf73afc78dec266/ffmpeg-python-0.2.0.tar.gz", hash = "sha256:65225db34627c578ef0e11c8b1eb528bb35e024752f6f10b78c011f6f64c4127", size = 21543 }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/d7/0c/56be52741f75bad4dc6555991fabd2e07b432d333da82c11ad701123888a/ffmpeg_python-0.2.0-py3-none-any.whl", hash = "sha256:ac441a0404e053f8b6a1113a77c0f452f1cfc62f6344a769475ffdc0f56c23c5", size = 25024 },
+]
+
 [[package]]
 name = "filelock"
 version = "3.19.1"
@@ -740,6 +752,8 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/ee/43/3cecdc0349359e1a527cbf2e3e28e5f8f06d3343aaf82ca13437a9aa290f/greenlet-3.2.4-cp313-cp313-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:23768528f2911bcd7e475210822ffb5254ed10d71f4028387e5a99b4c6699671", size = 610497 },
     { url = "https://files.pythonhosted.org/packages/b8/19/06b6cf5d604e2c382a6f31cafafd6f33d5dea706f4db7bdab184bad2b21d/greenlet-3.2.4-cp313-cp313-musllinux_1_1_aarch64.whl", hash = "sha256:00fadb3fedccc447f517ee0d3fd8fe49eae949e1cd0f6a611818f4f6fb7dc83b", size = 1121662 },
     { url = "https://files.pythonhosted.org/packages/a2/15/0d5e4e1a66fab130d98168fe984c509249c833c1a3c16806b90f253ce7b9/greenlet-3.2.4-cp313-cp313-musllinux_1_1_x86_64.whl", hash = "sha256:d25c5091190f2dc0eaa3f950252122edbbadbb682aa7b1ef2f8af0f8c0afefae", size = 1149210 },
+    { url = "https://files.pythonhosted.org/packages/1c/53/f9c440463b3057485b8594d7a638bed53ba531165ef0ca0e6c364b5cc807/greenlet-3.2.4-cp313-cp313-musllinux_1_2_aarch64.whl", hash = "sha256:6e343822feb58ac4d0a1211bd9399de2b3a04963ddeec21530fc426cc121f19b", size = 1564759 },
+    { url = "https://files.pythonhosted.org/packages/47/e4/3bb4240abdd0a8d23f4f88adec746a3099f0d86bfedb623f063b2e3b4df0/greenlet-3.2.4-cp313-cp313-musllinux_1_2_x86_64.whl", hash = "sha256:ca7f6f1f2649b89ce02f6f229d7c19f680a6238af656f61e0115b24857917929", size = 1634288 },
     { url = "https://files.pythonhosted.org/packages/0b/55/2321e43595e6801e105fcfdee02b34c0f996eb71e6ddffca6b10b7e1d771/greenlet-3.2.4-cp313-cp313-win_amd64.whl", hash = "sha256:554b03b6e73aaabec3745364d6239e9e012d64c68ccd0b8430c64ccc14939a8b", size = 299685 },
     { url = "https://files.pythonhosted.org/packages/22/5c/85273fd7cc388285632b0498dbbab97596e04b154933dfe0f3e68156c68c/greenlet-3.2.4-cp314-cp314-macosx_11_0_universal2.whl", hash = "sha256:49a30d5fda2507ae77be16479bdb62a660fa51b1eb4928b524975b3bde77b3c0", size = 273586 },
     { url = "https://files.pythonhosted.org/packages/d1/75/10aeeaa3da9332c2e761e4c50d4c3556c21113ee3f0afa2cf5769946f7a3/greenlet-3.2.4-cp314-cp314-manylinux2014_aarch64.manylinux_2_17_aarch64.whl", hash = "sha256:299fd615cd8fc86267b47597123e3f43ad79c9d8a22bebdce535e53550763e2f", size = 686346 },
@@ -747,6 +761,8 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/dc/8b/29aae55436521f1d6f8ff4e12fb676f3400de7fcf27fccd1d4d17fd8fecd/greenlet-3.2.4-cp314-cp314-manylinux2014_s390x.manylinux_2_17_s390x.whl", hash = "sha256:b4a1870c51720687af7fa3e7cda6d08d801dae660f75a76f3845b642b4da6ee1", size = 694659 },
     { url = "https://files.pythonhosted.org/packages/92/2e/ea25914b1ebfde93b6fc4ff46d6864564fba59024e928bdc7de475affc25/greenlet-3.2.4-cp314-cp314-manylinux2014_x86_64.manylinux_2_17_x86_64.whl", hash = "sha256:061dc4cf2c34852b052a8620d40f36324554bc192be474b9e9770e8c042fd735", size = 695355 },
     { url = "https://files.pythonhosted.org/packages/72/60/fc56c62046ec17f6b0d3060564562c64c862948c9d4bc8aa807cf5bd74f4/greenlet-3.2.4-cp314-cp314-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl", hash = "sha256:44358b9bf66c8576a9f57a590d5f5d6e72fa4228b763d0e43fee6d3b06d3a337", size = 657512 },
+    { url = "https://files.pythonhosted.org/packages/23/6e/74407aed965a4ab6ddd93a7ded3180b730d281c77b765788419484cdfeef/greenlet-3.2.4-cp314-cp314-musllinux_1_2_aarch64.whl", hash = "sha256:2917bdf657f5859fbf3386b12d68ede4cf1f04c90c3a6bc1f013dd68a22e2269", size = 1612508 },
+    { url = "https://files.pythonhosted.org/packages/0d/da/343cd760ab2f92bac1845ca07ee3faea9fe52bee65f7bcb19f16ad7de08b/greenlet-3.2.4-cp314-cp314-musllinux_1_2_x86_64.whl", hash = "sha256:015d48959d4add5d6c9f6c5210ee3803a830dce46356e3bc326d6776bde54681", size = 1680760 },
     { url = "https://files.pythonhosted.org/packages/e3/a5/6ddab2b4c112be95601c13428db1d8b6608a8b6039816f2ba09c346c08fc/greenlet-3.2.4-cp314-cp314-win_amd64.whl", hash = "sha256:e37ab26028f12dbb0ff65f29a8d3d44a765c61e729647bf2ddfbbed621726f01", size = 303425 },
 ]
 
@@ -1473,10 +1489,12 @@ version = "0.1.0"
 source = { editable = "." }
 dependencies = [
     { name = "click" },
+    { name = "ffmpeg-python" },
     { name = "mlflow" },
     { name = "neptune" },
     { name = "neptune-query" },
     { name = "pyarrow" },
+    { name = "python-magic" },
     { name = "tqdm" },
     { name = "wandb" },
 ]
@@ -1493,10 +1511,12 @@ dev = [
 [package.metadata]
 requires-dist = [
     { name = "click", specifier = ">=8.1.0,<9.0.0" },
+    { name = "ffmpeg-python", specifier = ">=0.2.0,<1.0.0" },
     { name = "mlflow", specifier = ">=3.4.0,<4.0.0" },
     { name = "neptune", specifier = ">=1.14.0,<2.0.0" },
     { name = "neptune-query", specifier = ">=1.5.1,<2.0.0" },
     { name = "pyarrow", specifier = ">=21.0.0,<22.0.0" },
+    { name = "python-magic", specifier = ">=0.4.27,<1.0.0" },
     { name = "tqdm", specifier = ">=4.67.1,<5.0.0" },
     { name = "wandb", specifier = ">=0.22.3,<1.0.0" },
 ]
@@ -2112,6 +2132,15 @@ wheels = [
     { url = "https://files.pythonhosted.org/packages/5f/ed/539768cf28c661b5b068d66d96a2f155c4971a5d55684a514c1a0e0dec2f/python_dotenv-1.1.1-py3-none-any.whl", hash = "sha256:31f23644fe2602f88ff55e1f5c79ba497e01224ee7737937930c448e4d0e24dc", size = 20556 },
 ]
 
+[[package]]
+name = "python-magic"
+version = "0.4.27"
+source = { registry = "https://pypi.org/simple" }
+sdist = { url = "https://files.pythonhosted.org/packages/da/db/0b3e28ac047452d079d375ec6798bf76a036a08182dbb39ed38116a49130/python-magic-0.4.27.tar.gz", hash = "sha256:c1ba14b08e4a5f5c31a302b7721239695b2f0f058d125bd5ce1ee36b9d9d3c3b", size = 14677 }
+wheels = [
+    { url = "https://files.pythonhosted.org/packages/6c/73/9f872cb81fc5c3bb48f7227872c28975f998f3e7c2b1c16e95e6432bbb90/python_magic-0.4.27-py2.py3-none-any.whl", hash = "sha256:c212960ad306f700aa0d01e5d7a325d20548ff97eb9920dcd29513174f0294d3", size = 13840 },
+]
+
 [[package]]
 name = "python-multipart"
 version = "0.0.20"
